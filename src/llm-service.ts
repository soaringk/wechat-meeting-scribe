import OpenAI from 'openai'
import { config } from './config.js'
import { LLMMessage, LLMResponse } from './types.js'

export class LLMService {
  private client: OpenAI
  private model: string

  constructor() {
    this.client = new OpenAI({
      apiKey: config.llmApiKey,
      baseURL: config.llmBaseUrl
    })
    this.model = config.llmModel
  }

  async chat(messages: LLMMessage[]): Promise<LLMResponse> {
    try {
      console.log(`[LLM] Sending request to ${this.model}...`)
      console.debug('[DEBUG] LLM Request:', messages)

      const response = await this.client.chat.completions.create({
        model: this.model,
        messages: messages
      })

      const content = response.choices[0]?.message?.content
      if (!content) {
        console.log('[LLM] No content in response')
        return { content: '' }
      }

      console.log(`[LLM] Response received (${content.length} chars)`)
      return { content }
    } catch (error) {
      console.error('[LLM] Error:', error)
      return {
        content: '',
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }

  async generateSummary(messages: string[]): Promise<string> {
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®è®°å½•åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç¾¤èŠæ¶ˆæ¯ç”Ÿæˆç®€æ´ã€ç»“æ„åŒ–çš„ä¼šè®®çºªè¦ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ä¼šè®®çºªè¦

### ğŸ“‹ å…³é”®è®¨è®ºç‚¹
- åˆ—å‡ºä¸»è¦è®¨è®ºçš„è¯é¢˜å’Œè§‚ç‚¹

### âœ… å†³å®šäº‹é¡¹
- åˆ—å‡ºè¾¾æˆçš„å†³å®šæˆ–å…±è¯†

### ğŸ“Œ å¾…åŠäº‹é¡¹
- åˆ—å‡ºéœ€è¦è·Ÿè¿›çš„è¡ŒåŠ¨é¡¹ï¼ˆå¦‚æœæœ‰æ˜ç¡®çš„è´Ÿè´£äººï¼Œè¯·æ ‡æ³¨ï¼‰

### ğŸ‘¥ ä¸»è¦å‚ä¸è€…
- åˆ—å‡ºæ´»è·ƒçš„å‘è¨€äºº

### ğŸ’¡ å…¶ä»–è¦ç‚¹
- å…¶ä»–å€¼å¾—è®°å½•çš„ä¿¡æ¯

æ³¨æ„ï¼š
1. ä¿æŒç®€æ´ï¼Œçªå‡ºé‡ç‚¹
2. ä½¿ç”¨ä¸­æ–‡
3. å¦‚æœæŸä¸ªéƒ¨åˆ†æ²¡æœ‰å†…å®¹ï¼Œå¯ä»¥çœç•¥
4. ä¿æŒå®¢è§‚ï¼Œä¸è¦æ·»åŠ ä¸ªäººè§‚ç‚¹`

    const conversationText = messages.join('\n')

    const llmMessages: LLMMessage[] = [
      { role: 'system', content: systemPrompt },
      {
        role: 'user',
        content: `è¯·ä¸ºä»¥ä¸‹ç¾¤èŠæ¶ˆæ¯ç”Ÿæˆä¼šè®®çºªè¦ï¼š\n\n${conversationText}`
      }
    ]

    const response = await this.chat(llmMessages)

    if (response.error) {
      throw new Error(`LLM service error: ${response.error}`)
    }

    return response.content
  }
}
