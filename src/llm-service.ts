import axios, { AxiosError } from 'axios'
import { config } from './config.js'
import { LLMMessage, LLMResponse } from './types.js'

export class LLMService {
  private apiUrl: string
  private apiKey: string
  private model: string

  constructor() {
    this.apiUrl = config.llmApiUrl
    this.apiKey = config.llmApiKey
    this.model = config.llmModel
  }

  async chat(messages: LLMMessage[]): Promise<LLMResponse> {
    try {
      console.log(`[LLM] Sending request to ${this.model}...`)

      const response = await axios.post(
        this.apiUrl,
        {
          model: this.model,
          messages: messages
        },
        {
          headers: {
            'Authorization': `Bearer ${this.apiKey}`,
            'Content-Type': 'application/json'
          },
          timeout: 60000
        }
      )

      if (response.data?.choices?.[0]?.message?.content) {
        const content = response.data.choices[0].message.content
        console.log(`[LLM] Response received (${content.length} chars)`)
        return { content }
      }

      throw new Error('Invalid response format from LLM API')
    } catch (error) {
      console.error('[LLM] Error:', error)

      if (axios.isAxiosError(error)) {
        const axiosError = error as AxiosError
        if (axiosError.response) {
          return {
            content: '',
            error: `API Error: ${axiosError.response.status} - ${JSON.stringify(axiosError.response.data)}`
          }
        } else if (axiosError.request) {
          return {
            content: '',
            error: 'Network error: No response from API'
          }
        }
      }

      return {
        content: '',
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }

  async generateSummary(messages: string[]): Promise<string> {
    const conversationText = messages.join('\n')

    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®è®°å½•åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç¾¤èŠæ¶ˆæ¯ç”Ÿæˆç®€æ´ã€ç»“æ„åŒ–çš„ä¼šè®®çºªè¦ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ä¼šè®®çºªè¦

### ğŸ“‹ å…³é”®è®¨è®ºç‚¹
- åˆ—å‡ºä¸»è¦è®¨è®ºçš„è¯é¢˜å’Œè§‚ç‚¹

### âœ… å†³å®šäº‹é¡¹
- åˆ—å‡ºè¾¾æˆçš„å†³å®šæˆ–å…±è¯†

### ğŸ“Œ å¾…åŠäº‹é¡¹
- åˆ—å‡ºéœ€è¦è·Ÿè¿›çš„è¡ŒåŠ¨é¡¹ï¼ˆå¦‚æœæœ‰æ˜ç¡®çš„è´Ÿè´£äººï¼Œè¯·æ ‡æ³¨ï¼‰

### ğŸ‘¥ ä¸»è¦å‚ä¸è€…
- åˆ—å‡ºæ´»è·ƒçš„å‘è¨€äºº

### ğŸ’¡ å…¶ä»–è¦ç‚¹
- å…¶ä»–å€¼å¾—è®°å½•çš„ä¿¡æ¯

æ³¨æ„ï¼š
1. ä¿æŒç®€æ´ï¼Œçªå‡ºé‡ç‚¹
2. ä½¿ç”¨ä¸­æ–‡
3. å¦‚æœæŸä¸ªéƒ¨åˆ†æ²¡æœ‰å†…å®¹ï¼Œå¯ä»¥çœç•¥
4. ä¿æŒå®¢è§‚ï¼Œä¸è¦æ·»åŠ ä¸ªäººè§‚ç‚¹`

    const llmMessages: LLMMessage[] = [
      { role: 'system', content: systemPrompt },
      {
        role: 'user',
        content: `è¯·ä¸ºä»¥ä¸‹ç¾¤èŠæ¶ˆæ¯ç”Ÿæˆä¼šè®®çºªè¦ï¼š\n\n${conversationText}`
      }
    ]

    const response = await this.chat(llmMessages)

    if (response.error) {
      throw new Error(`LLM service error: ${response.error}`)
    }

    return response.content
  }
}
